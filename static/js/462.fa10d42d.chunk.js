"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[462],{462:function(e,t,n){n.r(t);n(791),n(750);var s=n(184);t.default=function(){return(0,s.jsxs)("div",{className:"styled-project project-details-content",children:[(0,s.jsx)("h4",{children:"Overview"}),(0,s.jsxs)("p",{children:["This project investigates ",(0,s.jsx)("strong",{children:"semantic perception"})," where a mobile robot detects objects and forms a persistent, queryable memory of its environment. YOLOv8, a state-of-the-art neural network, interprets live image streams, while the robot\u2019s real-time pose situates each detection within the environment map. The system closes the loop between sensory input (\u201cwhat is here?\u201d), localization (\u201cwhere am I?\u201d), and semantic memory (\u201cwhat did I see here before?\u201d). By binding visual descriptors to spatial coordinates using ",(0,s.jsx)("strong",{children:"Qdrant"}),", retrieval of objects or scene contexts becomes possible via vector similarity enabling context-aware navigation and environment understanding."]}),(0,s.jsxs)("div",{className:"image-row",children:[(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/2_object_placement_in_maze.png",alt:"Object placement in maze environment",className:"scaled-image"}),(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/3_zoomed_in_placement_1.png",alt:"Zoomed-in: object placement",className:"scaled-image"}),(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/4_object_detection_placement_1.png",alt:"Detection: placement 1",className:"scaled-image"}),(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/5_object_detection_placement_2.png",alt:"Detection: placement 2",className:"scaled-image"}),(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/6_object_detection_placement_3.png",alt:"Detection: placement 3",className:"scaled-image"}),(0,s.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/7_detection_logs.png",alt:"Detection logs (ROS 2)",className:"scaled-image"})]}),(0,s.jsx)("div",{className:"caption",children:"Left: Objects arranged for detection in a maze. Center: YOLOv8 model identifies, localizes, and classifies objects at different locations. Right: Detection logs and output topics for downstream semantic mapping."}),(0,s.jsx)("h4",{children:"Theoretical Foundations"}),(0,s.jsxs)("p",{children:["YOLOv8 is trained to regress bounding boxes and class probabilities for all objects in a single pass. By coupling detection output with localization (AMCL pose estimates), each object sighting is turned into an ",(0,s.jsx)("strong",{children:"observation event"})," a tuple of spatial and perceptual features."]}),(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:"Visual embeddings (e.g., from a ResNet50 backbone) capture object appearance beyond class labels."}),(0,s.jsxs)("li",{children:["Position (",(0,s.jsx)("code",{children:"x, y, \u03b8"}),") encodes where in the environment this detection occurred."]}),(0,s.jsx)("li",{children:"Storing into Qdrant allows efficient semantic and spatial recall crucial for lifelong learning robots."})]}),(0,s.jsx)("h4",{children:"Significance"}),(0,s.jsxs)("ul",{children:[(0,s.jsx)("li",{children:"Enables map-aware searching: \u201cFind all detections of objects class X in region Y\u201d"}),(0,s.jsx)("li",{children:"Enables memory-based behaviors and semantic relocalization"}),(0,s.jsx)("li",{children:"Architecture extensible to new models, richer embedding spaces, and multi-robot collaboration"})]}),(0,s.jsx)("h4",{children:"Conclusion"}),(0,s.jsxs)("p",{children:[(0,s.jsx)("strong",{children:"turtlebot_3_ROSnav2__yolov8"})," demonstrates how modern vision (YOLOv8), localization, and semantic memory can be orchestrated to enable robots to see, remember, and reason about what is in the world and where."]})]})}},750:function(){}}]);
//# sourceMappingURL=462.fa10d42d.chunk.js.map