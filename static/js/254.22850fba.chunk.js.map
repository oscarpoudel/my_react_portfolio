{"version":3,"file":"static/js/254.22850fba.chunk.js","mappings":"kJA2GA,UApG8B,WAC5B,OACE,iBAAKA,UAAU,yCAAf,WAEE,+CACA,gIAEM,8EAFN,yMAMA,2BACE,2BAAI,gDAAJ,wDACA,2BAAI,iDAAJ,+DAGF,iDACA,2BACE,yGACA,+GACA,uGACA,+FAGF,0CACA,2BACE,2BAAI,wCAAJ,sDACA,2BAAI,yCAAJ,8BACA,2BAAI,0CACF,2BACE,sDACA,8CACA,uEAGJ,2BAAI,0DAAJ,kEAKF,yCACA,0FAGA,2BACE,2BAAI,iDAAJ,wDACA,2BAAI,kDAAJ,6DACA,uFAGF,qCACA,4FAGA,2BACE,2BAAI,kDACF,2BACE,gDACA,+CACA,mDACA,0DAGJ,2BAAI,mDACF,2BACE,mDACA,kDACA,sDACA,gEAIN,gBAAKC,IAAK,gGAAiGC,IAAI,8BAA8BF,UAAU,kBACvJ,gBAAKC,IAAK,gGAAiGC,IAAI,+BAA+BF,UAAU,kBAExJ,0CACA,2BACE,oGACA,gGACA,gHACA,+FAGF,sDACA,2BACE,uHACA,yGACA,sHAGF,0BAAG,4CAAH,0IAEyC,mEAFzC,uH","sources":["components/Detailslist/Project3.jsx"],"sourcesContent":["import React from 'react'\n// import viImage from \"../../assets/research_images/proj_recycling/1_value_iteration.png\"\n// import piImage from \"../../assets/research_images/proj_recycling/2_policy_iteration.png\"\n// import graph1 from \"../../assets/research_images/proj_recycling/3_transition_graph.png\"\n// import graph2 from \"../../assets/research_images/proj_recycling/4_mdp_states.png\"\nimport \"./Project1.scss\"\n\nconst ProjectRecyclingRobot = () => {\n  return (\n    <div className=\"project-details-content styled-project\">\n\n      <h4>Problem Statement</h4>\n      <p>\n        Efficient operation of autonomous recycling robots requires decision-making under uncertainty. \n        The <strong>Recycling Robot Markov Decision Process (MDP)</strong>, originally introduced by Sutton & Barto, \n        models the problem of managing a robot’s energy levels while performing recycling tasks. \n        Two classical reinforcement learning methods are studied:\n      </p>\n      <ul>\n        <li><strong>Value Iteration</strong> – based on the Bellman Optimality principle</li>\n        <li><strong>Policy Iteration</strong> – based on iterative evaluation and improvement</li>\n      </ul>\n\n      <h4>Research Objectives</h4>\n      <ul>\n        <li>Formulate the Recycling Robot MDP with defined states, actions, and rewards</li>\n        <li>Implement Value Iteration to compute the optimal value function and greedy policy</li>\n        <li>Apply Policy Iteration to evaluate and improve policies until convergence</li>\n        <li>Compare results and demonstrate convergence of both approaches</li>\n      </ul>\n\n      <h4>System Model</h4>\n      <ul>\n        <li><strong>States:</strong> High (battery charged), Low (battery depleted)</li>\n        <li><strong>Actions:</strong> Search, Wait, Recharge</li>\n        <li><strong>Rewards:</strong> \n          <ul>\n            <li>+1 for successful search</li>\n            <li>+0.2 for waiting</li>\n            <li>-3 penalty for failure in low state</li>\n          </ul>\n        </li>\n        <li><strong>Transition Probabilities:</strong> α (stay in high), β (return to high from low)</li>\n      </ul>\n      {/* <img src={graph1} alt=\"Transition Graph\" className=\"scaled-image\" />\n      <img src={graph2} alt=\"MDP State Diagram\" className=\"scaled-image\" /> */}\n\n      <h4>Methodology</h4>\n      <p>\n        The project applies classical dynamic programming algorithms:\n      </p>\n      <ul>\n        <li><strong>Value Iteration:</strong> Repeated Bellman updates until value convergence</li>\n        <li><strong>Policy Iteration:</strong> Alternating between policy evaluation and improvement</li>\n        <li>Convergence tracked by value updates across iterations</li>\n      </ul>\n\n      <h4>Results</h4>\n      <p>\n        Both methods converged to the same optimal values and policies:\n      </p>\n      <ul>\n        <li><strong>Value Iteration:</strong>\n          <ul>\n            <li>V(high) = 8.474576</li>\n            <li>V(low) = 7.627119</li>\n            <li>π(high) = search</li>\n            <li>π(low) = recharge</li>\n          </ul>\n        </li>\n        <li><strong>Policy Iteration:</strong>\n          <ul>\n            <li>V_pi(high) = 8.474576</li>\n            <li>V_pi(low) = 7.627119</li>\n            <li>π_pi(high) = search</li>\n            <li>π_pi(low) = recharge</li>\n          </ul>\n        </li>\n      </ul>\n      <img src={\"https://raw.githubusercontent.com/oscarpoudel/cleaning_robot_RL_VI-PI/main/pictures/res_1.png\"} alt=\"Value Iteration Convergence\" className=\"scaled-image\" />\n      <img src={\"https://raw.githubusercontent.com/oscarpoudel/cleaning_robot_RL_VI-PI/main/pictures/res_2.png\"} alt=\"Policy Iteration Convergence\" className=\"scaled-image\" />\n\n      <h4>Key Insights</h4>\n      <ul>\n        <li>Both methods achieve the same optimal strategy, confirming consistency</li>\n        <li>Value Iteration is straightforward but may require more iterations</li>\n        <li>Policy Iteration converges faster but requires full policy evaluation at each step</li>\n        <li>The optimal policy balances task reward with energy management</li>\n      </ul>\n\n      <h4>Practical Considerations</h4>\n      <ul>\n        <li>Extending the MDP to larger state spaces (e.g., multi-level battery) increases complexity</li>\n        <li>Real robots require approximations since exact transitions are rarely known</li>\n        <li>These foundational algorithms motivate reinforcement learning methods like Q-learning</li>\n      </ul>\n\n      <p><strong>Conclusion:</strong> \n        The Recycling Robot MDP experiment demonstrates that both Value Iteration and Policy Iteration \n        converge to the same optimal strategy: <em>search when charged, recharge when low</em>. \n        This highlights the strength of dynamic programming in solving decision-making problems \n        under uncertainty.\n      </p>\n    </div>\n  )\n}\n\nexport default ProjectRecyclingRobot\n"],"names":["className","src","alt"],"sourceRoot":""}