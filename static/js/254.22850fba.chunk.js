"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[254],{254:function(e,i,n){n.r(i);n(791),n(750);var r=n(184);i.default=function(){return(0,r.jsxs)("div",{className:"project-details-content styled-project",children:[(0,r.jsx)("h4",{children:"Problem Statement"}),(0,r.jsxs)("p",{children:["Efficient operation of autonomous recycling robots requires decision-making under uncertainty. The ",(0,r.jsx)("strong",{children:"Recycling Robot Markov Decision Process (MDP)"}),", originally introduced by Sutton & Barto, models the problem of managing a robot\u2019s energy levels while performing recycling tasks. Two classical reinforcement learning methods are studied:"]}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Value Iteration"})," \u2013 based on the Bellman Optimality principle"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Policy Iteration"})," \u2013 based on iterative evaluation and improvement"]})]}),(0,r.jsx)("h4",{children:"Research Objectives"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Formulate the Recycling Robot MDP with defined states, actions, and rewards"}),(0,r.jsx)("li",{children:"Implement Value Iteration to compute the optimal value function and greedy policy"}),(0,r.jsx)("li",{children:"Apply Policy Iteration to evaluate and improve policies until convergence"}),(0,r.jsx)("li",{children:"Compare results and demonstrate convergence of both approaches"})]}),(0,r.jsx)("h4",{children:"System Model"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"States:"})," High (battery charged), Low (battery depleted)"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Actions:"})," Search, Wait, Recharge"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Rewards:"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"+1 for successful search"}),(0,r.jsx)("li",{children:"+0.2 for waiting"}),(0,r.jsx)("li",{children:"-3 penalty for failure in low state"})]})]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Transition Probabilities:"})," \u03b1 (stay in high), \u03b2 (return to high from low)"]})]}),(0,r.jsx)("h4",{children:"Methodology"}),(0,r.jsx)("p",{children:"The project applies classical dynamic programming algorithms:"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Value Iteration:"})," Repeated Bellman updates until value convergence"]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Policy Iteration:"})," Alternating between policy evaluation and improvement"]}),(0,r.jsx)("li",{children:"Convergence tracked by value updates across iterations"})]}),(0,r.jsx)("h4",{children:"Results"}),(0,r.jsx)("p",{children:"Both methods converged to the same optimal values and policies:"}),(0,r.jsxs)("ul",{children:[(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Value Iteration:"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"V(high) = 8.474576"}),(0,r.jsx)("li",{children:"V(low) = 7.627119"}),(0,r.jsx)("li",{children:"\u03c0(high) = search"}),(0,r.jsx)("li",{children:"\u03c0(low) = recharge"})]})]}),(0,r.jsxs)("li",{children:[(0,r.jsx)("strong",{children:"Policy Iteration:"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"V_pi(high) = 8.474576"}),(0,r.jsx)("li",{children:"V_pi(low) = 7.627119"}),(0,r.jsx)("li",{children:"\u03c0_pi(high) = search"}),(0,r.jsx)("li",{children:"\u03c0_pi(low) = recharge"})]})]})]}),(0,r.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/cleaning_robot_RL_VI-PI/main/pictures/res_1.png",alt:"Value Iteration Convergence",className:"scaled-image"}),(0,r.jsx)("img",{src:"https://raw.githubusercontent.com/oscarpoudel/cleaning_robot_RL_VI-PI/main/pictures/res_2.png",alt:"Policy Iteration Convergence",className:"scaled-image"}),(0,r.jsx)("h4",{children:"Key Insights"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Both methods achieve the same optimal strategy, confirming consistency"}),(0,r.jsx)("li",{children:"Value Iteration is straightforward but may require more iterations"}),(0,r.jsx)("li",{children:"Policy Iteration converges faster but requires full policy evaluation at each step"}),(0,r.jsx)("li",{children:"The optimal policy balances task reward with energy management"})]}),(0,r.jsx)("h4",{children:"Practical Considerations"}),(0,r.jsxs)("ul",{children:[(0,r.jsx)("li",{children:"Extending the MDP to larger state spaces (e.g., multi-level battery) increases complexity"}),(0,r.jsx)("li",{children:"Real robots require approximations since exact transitions are rarely known"}),(0,r.jsx)("li",{children:"These foundational algorithms motivate reinforcement learning methods like Q-learning"})]}),(0,r.jsxs)("p",{children:[(0,r.jsx)("strong",{children:"Conclusion:"}),"The Recycling Robot MDP experiment demonstrates that both Value Iteration and Policy Iteration converge to the same optimal strategy: ",(0,r.jsx)("em",{children:"search when charged, recharge when low"}),". This highlights the strength of dynamic programming in solving decision-making problems under uncertainty."]})]})}},750:function(){}}]);
//# sourceMappingURL=254.22850fba.chunk.js.map