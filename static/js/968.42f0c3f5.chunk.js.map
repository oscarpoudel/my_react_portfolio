{"version":3,"file":"static/js/968.42f0c3f5.chunk.js","mappings":"kJAGMA,EAAU,4EAiJhB,UA/IuB,WACrB,OACE,iBAAKC,UAAU,yCAAf,WAEE,+CACA,wHAEE,2EAFF,wUAMA,iBAAKA,UAAU,YAAf,WACE,4BACE,gBAAKC,IAAKF,EAAU,uCAAwCG,IAAI,kCAAkCF,UAAU,kBAC5G,gBAAKA,UAAU,UAAf,4EAEF,4BACE,gBAAKC,IAAKF,EAAU,wBAAyBG,IAAI,yBAAyBF,UAAU,kBACpF,gBAAKA,UAAU,UAAf,qEAIJ,iDACA,2BACE,yHACA,wHACA,4GACA,iJACA,iGAGF,iBAAKA,UAAU,YAAf,WACE,4BACE,gBAAKC,IAAKF,EAAU,uDAAwDG,IAAI,6BAA6BF,UAAU,kBACvH,gBAAKA,UAAU,UAAf,mEAEF,4BACE,gBAAKC,IAAKF,EAAU,uCAAwCG,IAAI,yBAAyBF,UAAU,kBACnG,gBAAKA,UAAU,UAAf,2EAIJ,0CACA,2BACE,2BACE,gDADF,+DAGA,2BACE,mDADF,2JAGA,2BACE,4CADF,yIAGA,2BACE,6CADF,qGAGA,2BACE,gDADF,kIAKF,gBAAKA,UAAU,YAAf,UACE,4BACE,gBAAKC,IAAKF,EAAU,4BAA6BG,IAAI,iBAAiBF,UAAU,kBAChF,gBAAKA,UAAU,UAAf,uEAIJ,yCACA,4fAGA,2BACE,2BAAI,+EAAJ,kDACA,2BAAI,uDAAJ,8FACA,2BACE,+CADF,sJAKF,iBAAKA,UAAU,YAAf,WACE,4BACE,gBAAKC,IAAKF,EAAU,+DAAgEG,IAAI,+BAA+BF,UAAU,kBACjI,gBAAKA,UAAU,UAAf,gFAEF,4BACE,gBAAKC,IAAKF,EAAU,mCAAoCG,IAAI,2BAA2BF,UAAU,kBACjG,gBAAKA,UAAU,UAAf,uDAIJ,iBAAKA,UAAU,YAAf,WACE,4BACE,gBAAKC,IAAKF,EAAU,6BAA8BG,IAAI,iBAAiBF,UAAU,kBACjF,gBAAKA,UAAU,UAAf,8DAEF,4BACE,gBAAKC,IAAKF,EAAU,4CAA6CG,IAAI,8BAA8BF,UAAU,kBAC7G,gBAAKA,UAAU,UAAf,gFAIJ,qCACA,2BACE,gMACA,8HACA,0IAGF,0CACA,2BACE,sJAGA,sHAGA,wLAKF,sDACA,2BACE,6JAGA,mMAGA,sMAKF,0BACE,4CADF,4P","sources":["components/Detailslist/Project8.jsx"],"sourcesContent":["import React from 'react'\r\nimport \"./Project1.scss\"\r\n\r\nconst imgBase = \"https://raw.githubusercontent.com/oscarpoudel/LLM_ROS2_agent/main/images/\";\r\n\r\nconst ProjectLLMROS2 = () => {\r\n  return (\r\n    <div className=\"project-details-content styled-project\">\r\n\r\n      <h4>Problem Statement</h4>\r\n      <p>\r\n        Direct, intuitive control of robots by everyday users is a major hurdle in modern robotics.\r\n        <strong> ROS-LM: Natural Language Robot Assistant </strong> addresses this challenge by using a Large Language Model (LLM) agent to interpret and execute natural language commands, enabling seamless interaction with a ROS2-powered mobile robot.\r\n        This system removes the need for complex ROS2 commands and scripts, allowing real-world actions to be triggered by simple conversation.\r\n      </p>\r\n\r\n      <div className=\"image-row\">\r\n        <div>\r\n          <img src={imgBase + \"1.gui_layout_for_llm_promt_entry.png\"} alt=\"GUI layout for LLM prompt entry\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Streamlit GUI for entering natural language robot commands</div>\r\n        </div>\r\n        <div>\r\n          <img src={imgBase + \"2.cli_based_entry.png\"} alt=\"CLI-based prompt entry\" className=\"scaled-image\" />\r\n          <div className=\"caption\">CLI-based natural language interaction interface</div>\r\n        </div>\r\n      </div>\r\n\r\n      <h4>Research Objectives</h4>\r\n      <ul>\r\n        <li>Enable LLM-powered interpretation and orchestration of user instructions for mobile robots.</li>\r\n        <li>Leverage agentic AI principles, with LLM-driven intent classification and tool delegation.</li>\r\n        <li>Integrate NATS messaging for scalable, real-time, and decoupled robot control.</li>\r\n        <li>Support multimodal interaction (motion, navigation, vision, diagnostics, ROS2 CLI) through extensible tool modules.</li>\r\n        <li>Deliver accessible interfaces via both CLI and Streamlit web UI.</li>\r\n      </ul>\r\n\r\n      <div className=\"image-row\">\r\n        <div>\r\n          <img src={imgBase + \"3.more_cli_based_output_performing_vision_anysis.png\"} alt=\"Vision analysis CLI output\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Vision analysis performed from CLI via LLM prompt</div>\r\n        </div>\r\n        <div>\r\n          <img src={imgBase + \"4_system_status_output_with_nats.png\"} alt=\"System status via NATS\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Live robot system status feedback using NATS messaging</div>\r\n        </div>\r\n      </div>\r\n\r\n      <h4>System Model</h4>\r\n      <ul>\r\n        <li>\r\n          <strong>User Interface:</strong> Accepts natural language commands from web or terminal.\r\n        </li>\r\n        <li>\r\n          <strong>LLM-Powered Agent:</strong> Classifies intent (motion, nav, vision, status, CLI) and extracts parameters by reasoning over free-form text using powerful LLMs (Ollama, Gemini).\r\n        </li>\r\n        <li>\r\n          <strong>Tool Suite:</strong> Modular Python tools handle domain-specific logic (motion, navigation, etc.), packaging instructions as structured NATS messages.\r\n        </li>\r\n        <li>\r\n          <strong>NATS Broker:</strong> Publishes/subscribes agent commands and robot responses for reliable real-time communication.\r\n        </li>\r\n        <li>\r\n          <strong>ROS2 Execution:</strong> ROS2 nodes receive and execute instructions, returning feedback and results for the LLM agent to summarize to the user.\r\n        </li>\r\n      </ul>\r\n\r\n      <div className=\"image-row\">\r\n        <div>\r\n          <img src={imgBase + \"5.some_prompt_samples.png\"} alt=\"Prompt samples\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Various prompt samples for LLM-driven robot actions</div>\r\n        </div>\r\n      </div>\r\n\r\n      <h4>Methodology</h4>\r\n      <p>\r\n        The agentic orchestration relies on the LLM to act as the central “brain,” analyzing incoming language, classifying intent, and invoking tool modules. Each tool serializes its task as a NATS message, enabling robust, decoupled integration with the ROS2 backend. Multimodal queries such as “navigate to the lab,” “move forward 2 meters,” or “what do you see?” are parsed and executed via their corresponding tool. The architecture supports:\r\n      </p>\r\n      <ul>\r\n        <li><strong>Intent classification and parameter extraction</strong> powered by LLM reasoning and tool-calling.</li>\r\n        <li><strong>Agentic orchestration:</strong> Each tool invocation acts as a step in an autonomous decision-making and action chain.</li>\r\n        <li>\r\n          <strong>Feedback loop:</strong> Robot state, vision, and diagnostics are integrated into the LLM’s conversational loop, enabling clarification or multi-step planning.\r\n        </li>\r\n      </ul>\r\n\r\n      <div className=\"image-row\">\r\n        <div>\r\n          <img src={imgBase + \"6.moving_robot-based_on_symantic_locaiton_eg_firehydrant.png\"} alt=\"Semantic-location navigation\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Moving the robot to a semantic location (e.g., \"fire hydrant\")</div>\r\n        </div>\r\n        <div>\r\n          <img src={imgBase + \"7.path_planned_based_on_nav2.png\"} alt=\"Nav2-based path planning\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Path planned using ROS2 Nav2 stack</div>\r\n        </div>\r\n      </div>\r\n\r\n      <div className=\"image-row\">\r\n        <div>\r\n          <img src={imgBase + \"8.ros2_nats_side_codes.png\"} alt=\"ROS2-NATS code\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Example: ROS2 and NATS message handling code</div>\r\n        </div>\r\n        <div>\r\n          <img src={imgBase + \"9.running_nats_receiver_and_ros2_maze.png\"} alt=\"NATS receiver and ROS2 maze\" className=\"scaled-image\" />\r\n          <div className=\"caption\">Running NATS receiver along with ROS2-based maze navigation</div>\r\n        </div>\r\n      </div>\r\n\r\n      <h4>Results</h4>\r\n      <ul>\r\n        <li>Validated natural language interaction for moving, navigating, querying vision, checking status, and running ROS2 CLI commands on Turtlebot3 with ROS2 Nav2 stack.</li>\r\n        <li>Streamlit UI and CLI both allow conversational robot programming and immediate action execution.</li>\r\n        <li>Scalable design supports easy tool/toolchain extension any new ROS2 action can be added as a tool module.</li>\r\n      </ul>\r\n\r\n      <h4>Key Insights</h4>\r\n      <ul>\r\n        <li>\r\n          The agentic AI approach enables flexible, multi-intent orchestration far exceeding rule-based or menu-driven approaches.\r\n        </li>\r\n        <li>\r\n          NATS messaging provides robust, scalable decoupling of language-agent and robot systems.\r\n        </li>\r\n        <li>\r\n          LLM acts as the orchestrator for high-level planning, not just intent detection, making multi-step natural language control both robust and extensible.\r\n        </li>\r\n      </ul>\r\n\r\n      <h4>Practical Considerations</h4>\r\n      <ul>\r\n        <li>\r\n          The system’s modularity accelerates robot platform adaptation just edit or extend tool modules for new hardware or topics.\r\n        </li>\r\n        <li>\r\n          LLM prompt and tool-calling design require iterative tweaking for ambiguous or complex instructions; robust error handling ensures user guidance is always available.\r\n        </li>\r\n        <li>\r\n          Orchestrated agentic control supports not just single actions, but can plan and execute chains of commands, opening doors to more advanced human-robot collaboration.\r\n        </li>\r\n      </ul>\r\n\r\n      <p>\r\n        <strong>Conclusion:</strong>\r\n        ROS-LM demonstrates practical, LLM-based agentic orchestration for ROS2 robots bridging natural language and real-world action with scalable, modular, and robust design, paving the way for accessible and adaptive robot control for all users.\r\n      </p>\r\n    </div>\r\n  )\r\n}\r\n\r\nexport default ProjectLLMROS2\r\n"],"names":["imgBase","className","src","alt"],"sourceRoot":""}