"use strict";(self.webpackChunkmy_portfolio=self.webpackChunkmy_portfolio||[]).push([[968],{968:function(e,a,s){s.r(a);s(791),s(750);var n=s(184),i="https://raw.githubusercontent.com/oscarpoudel/LLM_ROS2_agent/main/images/";a.default=function(){return(0,n.jsxs)("div",{className:"project-details-content styled-project",children:[(0,n.jsx)("h4",{children:"Problem Statement"}),(0,n.jsxs)("p",{children:["Direct, intuitive control of robots by everyday users is a major hurdle in modern robotics.",(0,n.jsx)("strong",{children:" ROS-LM: Natural Language Robot Assistant "})," addresses this challenge by using a Large Language Model (LLM) agent to interpret and execute natural language commands, enabling seamless interaction with a ROS2-powered mobile robot. This system removes the need for complex ROS2 commands and scripts, allowing real-world actions to be triggered by simple conversation."]}),(0,n.jsxs)("div",{className:"image-row",children:[(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"1.gui_layout_for_llm_promt_entry.png",alt:"GUI layout for LLM prompt entry",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Streamlit GUI for entering natural language robot commands"})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"2.cli_based_entry.png",alt:"CLI-based prompt entry",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"CLI-based natural language interaction interface"})]})]}),(0,n.jsx)("h4",{children:"Research Objectives"}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"Enable LLM-powered interpretation and orchestration of user instructions for mobile robots."}),(0,n.jsx)("li",{children:"Leverage agentic AI principles, with LLM-driven intent classification and tool delegation."}),(0,n.jsx)("li",{children:"Integrate NATS messaging for scalable, real-time, and decoupled robot control."}),(0,n.jsx)("li",{children:"Support multimodal interaction (motion, navigation, vision, diagnostics, ROS2 CLI) through extensible tool modules."}),(0,n.jsx)("li",{children:"Deliver accessible interfaces via both CLI and Streamlit web UI."})]}),(0,n.jsxs)("div",{className:"image-row",children:[(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"3.more_cli_based_output_performing_vision_anysis.png",alt:"Vision analysis CLI output",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Vision analysis performed from CLI via LLM prompt"})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"4_system_status_output_with_nats.png",alt:"System status via NATS",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Live robot system status feedback using NATS messaging"})]})]}),(0,n.jsx)("h4",{children:"System Model"}),(0,n.jsxs)("ul",{children:[(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"User Interface:"})," Accepts natural language commands from web or terminal."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"LLM-Powered Agent:"})," Classifies intent (motion, nav, vision, status, CLI) and extracts parameters by reasoning over free-form text using powerful LLMs (Ollama, Gemini)."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"Tool Suite:"})," Modular Python tools handle domain-specific logic (motion, navigation, etc.), packaging instructions as structured NATS messages."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"NATS Broker:"})," Publishes/subscribes agent commands and robot responses for reliable real-time communication."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"ROS2 Execution:"})," ROS2 nodes receive and execute instructions, returning feedback and results for the LLM agent to summarize to the user."]})]}),(0,n.jsx)("div",{className:"image-row",children:(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"5.some_prompt_samples.png",alt:"Prompt samples",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Various prompt samples for LLM-driven robot actions"})]})}),(0,n.jsx)("h4",{children:"Methodology"}),(0,n.jsx)("p",{children:"The agentic orchestration relies on the LLM to act as the central \u201cbrain,\u201d analyzing incoming language, classifying intent, and invoking tool modules. Each tool serializes its task as a NATS message, enabling robust, decoupled integration with the ROS2 backend. Multimodal queries such as \u201cnavigate to the lab,\u201d \u201cmove forward 2 meters,\u201d or \u201cwhat do you see?\u201d are parsed and executed via their corresponding tool. The architecture supports:"}),(0,n.jsxs)("ul",{children:[(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"Intent classification and parameter extraction"})," powered by LLM reasoning and tool-calling."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"Agentic orchestration:"})," Each tool invocation acts as a step in an autonomous decision-making and action chain."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)("strong",{children:"Feedback loop:"})," Robot state, vision, and diagnostics are integrated into the LLM\u2019s conversational loop, enabling clarification or multi-step planning."]})]}),(0,n.jsxs)("div",{className:"image-row",children:[(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"6.moving_robot-based_on_symantic_locaiton_eg_firehydrant.png",alt:"Semantic-location navigation",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:'Moving the robot to a semantic location (e.g., "fire hydrant")'})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"7.path_planned_based_on_nav2.png",alt:"Nav2-based path planning",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Path planned using ROS2 Nav2 stack"})]})]}),(0,n.jsxs)("div",{className:"image-row",children:[(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"8.ros2_nats_side_codes.png",alt:"ROS2-NATS code",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Example: ROS2 and NATS message handling code"})]}),(0,n.jsxs)("div",{children:[(0,n.jsx)("img",{src:i+"9.running_nats_receiver_and_ros2_maze.png",alt:"NATS receiver and ROS2 maze",className:"scaled-image"}),(0,n.jsx)("div",{className:"caption",children:"Running NATS receiver along with ROS2-based maze navigation"})]})]}),(0,n.jsx)("h4",{children:"Results"}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"Validated natural language interaction for moving, navigating, querying vision, checking status, and running ROS2 CLI commands on Turtlebot3 with ROS2 Nav2 stack."}),(0,n.jsx)("li",{children:"Streamlit UI and CLI both allow conversational robot programming and immediate action execution."}),(0,n.jsx)("li",{children:"Scalable design supports easy tool/toolchain extension any new ROS2 action can be added as a tool module."})]}),(0,n.jsx)("h4",{children:"Key Insights"}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"The agentic AI approach enables flexible, multi-intent orchestration far exceeding rule-based or menu-driven approaches."}),(0,n.jsx)("li",{children:"NATS messaging provides robust, scalable decoupling of language-agent and robot systems."}),(0,n.jsx)("li",{children:"LLM acts as the orchestrator for high-level planning, not just intent detection, making multi-step natural language control both robust and extensible."})]}),(0,n.jsx)("h4",{children:"Practical Considerations"}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"The system\u2019s modularity accelerates robot platform adaptation just edit or extend tool modules for new hardware or topics."}),(0,n.jsx)("li",{children:"LLM prompt and tool-calling design require iterative tweaking for ambiguous or complex instructions; robust error handling ensures user guidance is always available."}),(0,n.jsx)("li",{children:"Orchestrated agentic control supports not just single actions, but can plan and execute chains of commands, opening doors to more advanced human-robot collaboration."})]}),(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{children:"Conclusion:"}),"ROS-LM demonstrates practical, LLM-based agentic orchestration for ROS2 robots bridging natural language and real-world action with scalable, modular, and robust design, paving the way for accessible and adaptive robot control for all users."]})]})}},750:function(){}}]);
//# sourceMappingURL=968.42f0c3f5.chunk.js.map