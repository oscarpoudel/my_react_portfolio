{"version":3,"file":"static/js/780.3da4d6dd.chunk.js","mappings":"kJAoFA,UAjFqB,WACnB,OACE,iBAAKA,UAAU,yCAAf,WACE,+CACA,oHAEM,4CAFN,2KAIA,2BACE,0FACA,kGAGF,iDACA,2BACE,6GACA,iGACA,kGAGF,0CACA,2BACE,2BAAI,uCAAJ,gFACA,2BAAI,sDACF,2BACE,+DACA,sFACA,6EACA,gGAGJ,2BAAI,wCAAJ,mFAGF,yCACA,qPAGA,2BACE,iEACA,oEACA,yFACA,yFAGF,qCACA,+LAGA,2BACE,sHAGA,uHAIF,gBAAKC,IAAI,6EAA6EC,IAAI,2BAA2BF,UAAU,kBAC/H,gBAAKC,IAAI,6EAA6EC,IAAI,0CAA0CF,UAAU,kBAE9I,0CACA,2BACE,yGACA,mHACA,8FAGF,sDACA,2BACE,yHACA,qGACA,qGAGF,0BAAG,4CAAH,6L","sources":["components/Detailslist/Project4.jsx"],"sourcesContent":["import React from 'react'\r\nimport \"./Project1.scss\"\r\n\r\nconst ProjectVSLAM = () => {\r\n  return (\r\n    <div className=\"project-details-content styled-project\">\r\n      <h4>Problem Statement</h4>\r\n      <p>\r\n        Accurate real-world navigation requires estimating motion from camera video alone.\r\n        The <strong>VSLAM_light</strong> project implements a simple monocular visual odometry pipeline, aiming for robust pose estimation from image sequences using feature tracking and Kalman filtering.\r\n      </p>\r\n      <ul>\r\n        <li>Estimate real-time camera trajectories from monocular images</li>\r\n        <li>Use only consumer-level datasets, hardware, and open-source tools</li>\r\n      </ul>\r\n\r\n      <h4>Research Objectives</h4>\r\n      <ul>\r\n        <li>Develop a minimal visual odometry pipeline leveraging classical computer vision</li>\r\n        <li>Apply and compare feature tracking algorithms and smoothing filters</li>\r\n        <li>Support evaluation against ground-truth and KITTI-format datasets</li>\r\n      </ul>\r\n\r\n      <h4>System Model</h4>\r\n      <ul>\r\n        <li><strong>Input:</strong> Monocular image stream, camera calibration file, ground-truth trajectory</li>\r\n        <li><strong>Processing pipeline:</strong>\r\n          <ul>\r\n            <li>Corner detection using Shi-Tomasi</li>\r\n            <li>Feature tracking via pyramidal Lucas-Kanade optical flow</li>\r\n            <li>Pose estimation by essential matrix calculation</li>\r\n            <li>Trajectory smoothing using a constant-velocity Kalman filter</li>\r\n          </ul>\r\n        </li>\r\n        <li><strong>Output:</strong> Live plots of feature matches and estimated vs ground-truth trajectories</li>\r\n      </ul>\r\n\r\n      <h4>Methodology</h4>\r\n      <p>\r\n        The pipeline sequentially detects/filters features, tracks them across frames, and estimates 3D motion increments. Kalman filtering improves trajectory quality by integrating prior motion models with noisy estimates.\r\n      </p>\r\n      <ul>\r\n        <li>Robust corner detection and pruning</li>\r\n        <li>Optical flow tracking (frame-to-frame)</li>\r\n        <li>Relative pose estimation via essential matrix decomposition</li>\r\n        <li>Constant-velocity Kalman filter for trajectory smoothing</li>\r\n      </ul>\r\n\r\n      <h4>Results</h4>\r\n      <p>\r\n        The system accurately tracks and plots the estimated camera path against ground truth, showing clear visualizations of tracked features and trajectory comparison.\r\n      </p>\r\n      <ul>\r\n        <li>\r\n          Error is minimized with Kalman filtering, providing smoother and more reliable estimates\r\n        </li>\r\n        <li>\r\n          Real-time visualizations support live debugging and assessment of tracking performance\r\n        </li>\r\n      </ul>\r\n      <img src=\"https://raw.githubusercontent.com/oscarpoudel/VSLAM_light/main/image/2.png\" alt=\"Feature tracking overlay\" className=\"scaled-image\" />\r\n      <img src=\"https://raw.githubusercontent.com/oscarpoudel/VSLAM_light/main/image/1.png\" alt=\"Estimated and ground-truth trajectories\" className=\"scaled-image\" />\r\n\r\n      <h4>Key Insights</h4>\r\n      <ul>\r\n        <li>Classical feature-based odometry remains powerful for well-behaved datasets</li>\r\n        <li>Kalman filtering provides significant improvements in estimated trajectory smoothness</li>\r\n        <li>Live visualization helps tune and debug each processing stage</li>\r\n      </ul>\r\n\r\n      <h4>Practical Considerations</h4>\r\n      <ul>\r\n        <li>Monocular VO cannot recover scale in absolute termsâ€”ground-truth needed for evaluation</li>\r\n        <li>Sensitivity to lighting and scene structure may affect feature tracking</li>\r\n        <li>Real deployment needs outlier rejection and possibly inertial fusion</li>\r\n      </ul>\r\n\r\n      <p><strong>Conclusion:</strong>\r\n        VSLAM_light shows a practical, approachable, and open-source baseline for visual odometry research, supporting education, prototyping, and rapid development on standard datasets.\r\n      </p>\r\n    </div>\r\n  )\r\n}\r\n\r\nexport default ProjectVSLAM\r\n"],"names":["className","src","alt"],"sourceRoot":""}