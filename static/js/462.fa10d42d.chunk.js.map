{"version":3,"file":"static/js/462.fa10d42d.chunk.js","mappings":"kJAiDA,UA9C+B,WAC7B,OACE,iBAAKA,UAAU,yCAAf,WACE,sCACA,uDAC4B,oDAD5B,2fAEoN,uCAFpN,uJAKA,iBAAKA,UAAU,YAAf,WACE,gBAAKC,IAAI,uHAAuHC,IAAI,uCAAuCF,UAAU,kBACrL,gBAAKC,IAAI,oHAAoHC,IAAI,8BAA8BF,UAAU,kBACzK,gBAAKC,IAAI,2HAA2HC,IAAI,yBAAyBF,UAAU,kBAC3K,gBAAKC,IAAI,2HAA2HC,IAAI,yBAAyBF,UAAU,kBAC3K,gBAAKC,IAAI,2HAA2HC,IAAI,yBAAyBF,UAAU,kBAC3K,gBAAKC,IAAI,6GAA6GC,IAAI,yBAAyBF,UAAU,qBAE/J,gBAAKA,UAAU,UAAf,kOAIA,qDACA,gPACqN,kDADrN,mDAGA,2BACE,+HACA,wCAAc,2CAAd,kEACA,uIAGF,0CACA,2BACE,yHACA,wFACA,8HAGF,wCACA,0BACE,4DADF,8L","sources":["components/Detailslist/Project7.jsx"],"sourcesContent":["import React from 'react'\r\nimport \"./Project1.scss\"\r\n\r\nconst ProjectTurtlebotYolov8 = () => {\r\n  return (\r\n    <div className=\"styled-project project-details-content\">\r\n      <h4>Overview</h4>\r\n      <p>\r\n        This project investigates <strong>semantic perception</strong> where a mobile robot detects objects and forms a persistent, queryable memory of its environment. YOLOv8, a state-of-the-art neural network, interprets live image streams, while the robot’s real-time pose situates each detection within the environment map.\r\n        The system closes the loop between sensory input (“what is here?”), localization (“where am I?”), and semantic memory (“what did I see here before?”). By binding visual descriptors to spatial coordinates using <strong>Qdrant</strong>, retrieval of objects or scene contexts becomes possible via vector similarity enabling context-aware navigation and environment understanding.\r\n      </p>\r\n\r\n      <div className=\"image-row\">\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/2_object_placement_in_maze.png\" alt=\"Object placement in maze environment\" className=\"scaled-image\"/>\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/3_zoomed_in_placement_1.png\" alt=\"Zoomed-in: object placement\" className=\"scaled-image\"/>\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/4_object_detection_placement_1.png\" alt=\"Detection: placement 1\" className=\"scaled-image\"/>\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/5_object_detection_placement_2.png\" alt=\"Detection: placement 2\" className=\"scaled-image\"/>\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/6_object_detection_placement_3.png\" alt=\"Detection: placement 3\" className=\"scaled-image\"/>\r\n        <img src=\"https://raw.githubusercontent.com/oscarpoudel/turtlebot_3_ROSnav2__yolov8/main/images/7_detection_logs.png\" alt=\"Detection logs (ROS 2)\" className=\"scaled-image\"/>\r\n      </div>\r\n      <div className=\"caption\">\r\n        Left: Objects arranged for detection in a maze. Center: YOLOv8 model identifies, localizes, and classifies objects at different locations. Right: Detection logs and output topics for downstream semantic mapping.\r\n      </div>\r\n\r\n      <h4>Theoretical Foundations</h4>\r\n      <p>\r\n        YOLOv8 is trained to regress bounding boxes and class probabilities for all objects in a single pass. By coupling detection output with localization (AMCL pose estimates), each object sighting is turned into an <strong>observation event</strong> a tuple of spatial and perceptual features.\r\n      </p>\r\n      <ul>\r\n        <li>Visual embeddings (e.g., from a ResNet50 backbone) capture object appearance beyond class labels.</li>\r\n        <li>Position (<code>x, y, θ</code>) encodes where in the environment this detection occurred.</li>\r\n        <li>Storing into Qdrant allows efficient semantic and spatial recall crucial for lifelong learning robots.</li>\r\n      </ul>\r\n\r\n      <h4>Significance</h4>\r\n      <ul>\r\n        <li>Enables map-aware searching: “Find all detections of objects class X in region Y”</li>\r\n        <li>Enables memory-based behaviors and semantic relocalization</li>\r\n        <li>Architecture extensible to new models, richer embedding spaces, and multi-robot collaboration</li>\r\n      </ul>\r\n\r\n      <h4>Conclusion</h4>\r\n      <p>\r\n        <strong>turtlebot_3_ROSnav2__yolov8</strong> demonstrates how modern vision (YOLOv8), localization, and semantic memory can be orchestrated to enable robots to see, remember, and reason about what is in the world and where.\r\n      </p>\r\n    </div>\r\n  )\r\n}\r\n\r\nexport default ProjectTurtlebotYolov8\r\n"],"names":["className","src","alt"],"sourceRoot":""}